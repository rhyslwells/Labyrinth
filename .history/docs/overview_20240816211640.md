# Overview

Inspired by https://en.wikipedia.org/wiki/Labyrinth and [OpenAI Plays Hide and Seekâ€¦and Breaks The Game! ðŸ¤–](https://www.youtube.com/watch?v=Lu56xVlZ40M&t=35s)

The project aims to model and simulate a pursuit dynamic between a hunter and prey in a 2D environment. We aim to use reinforcement learning to achieve this, and explore the resulting data of this learning process.

The goals of the project are to:


- Explore the emergent dynamics of actors with the setting.
- Analyse the outcomes of games over time.
- Provide a reasonable simulation of the scenario.
- Be able to run many simulations.
- Provide a visualisation for a given Game

To keep this project manageable we want to:

- Want to remain in Python and display solutions via Jupyter notebook.

# A Game Instance

A Game will consist of Actors within an Environment. 

A Game ends if an Actor reaches their objective, and a loss for all other Actors.

A Game can be visualised. In particular the Environment and Actors.

For each time step of a Game we record the position of Actors within the Environment in a Table.

Game statistics such as success rate, which actor wins, average time to catch, and escape rates, will be recorded for future analysis.

# Entities

As the movement dynamics and characteristics of both hunter and prey are similar. They will inherit from a parent class called Actor.

The Actor class:

- Focuses movement characteristics such as position and speed,
- Size and shape of actor.
- Actors will explore towards their objective.
- Actors can change speeds dynamically during the game. There is a fixed maximum speed they can attain.
- To navigate the actors will scan, make a decision, then travel in the direction of the decision. They will do this in discrete steps (aim to do this continuously).

The **Hunter** subclass:
- Moves towards the **Prey**
- The hunterâ€™s objective is to get to the **Prey**.
- If the hunter sees the prey (during a scan), the **Hunter** will want to minimize the distance from **Prey**.

The **Prey** subclass:
- The prey's objective is to get to the exit.
- If seen (during a scan) by the **Hunter** the **Prey** will want to maximize the distance from hunter
- The prey may approach the **Hunter** if it increases the total distance in the long run.

# Environment of a Game

Each Game will take place within an Environment. The Actorâ€™s will navigate within the Environment 

The Environment will have boundaries and obstacles that the Actorâ€™s interact with and can not pass through.

# Project Overview: Hunter and Prey Simulation

Introduction
This project aims to model and simulate a dynamic pursuit scenario between a hunter and prey within a 2D environment using reinforcement learning (RL). The objective is to explore how different RL strategies can be employed to model the behaviors of both the hunter and prey, allowing them to adapt and optimize their actions over time. Through this, we aim to analyze the resulting data to understand the dynamics of pursuit and evasion, the efficiency of learning algorithms, and the emergent behaviors in such a setting.

Modeling the Pursuit Dynamics
The pursuit dynamics can be framed as a Markov Decision Process (MDP), where the outcome (next state and reward) depends only on the current state and action, not on previous states. The MDP framework allows us to systematically model and analyze the decision-making processes of both the hunter and prey.

Hunter's Goal: Learn a policy that maximizes the probability of capturing the prey in the shortest possible time.
Prey's Goal: Learn a policy that maximizes the likelihood of evading the hunter for the longest possible time.
Learning Algorithms
Different reinforcement learning algorithms will be implemented and tested to achieve the goals of the hunter and prey:

Q-Learning: A value-based, model-free RL algorithm where the agent learns the optimal policy by updating Q-values based on received rewards. It is particularly useful in discrete environments like our 2D grid. Q-learning is an off-policy algorithm.

Deep Q-Networks (DQN): An extension of Q-Learning that uses deep neural networks to approximate Q-values, suitable for handling larger state spaces or more complex environments.

SARSA (State-Action-Reward-State-Action): Another value-based RL algorithm that updates the Q-values using the action actually taken by the policy, providing a more on-policy learning approach compared to Q-Learning.

Policy Gradient Methods: These methods directly optimize the policy by adjusting the parameters based on the gradient of expected rewards. They are effective in environments where the action space is continuous or when modeling complex behaviors. Policies map states to action probabilities, while value functions estimate the expected return, guiding the agentâ€™s decisions to maximize rewards.

Exploration vs. Exploitation
A key challenge in this project is balancing exploration (trying new actions to discover more about the environment) and exploitation (choosing the best-known actions to maximize rewards). This balance is critical to ensuring that the hunter efficiently learns to catch the prey while the prey learns to evade the hunter. The agent will have to endure many penalties before the solution is found.

Epsilon: Also known as the exploration factor, is a crucial component of the Q-learning algorithm.
Data Analysis
The resulting data from the learning process will be analyzed to observe:

Convergence: How quickly and effectively the agents learn optimal policies.
Emergent Behaviors: Unexpected strategies or patterns that arise from the interaction between the hunter and prey.
Learning Efficiency: The impact of different reward structures, exploration strategies, and learning algorithms on the speed and quality of learning.
Tools and Resources
Libraries and Python Frameworks: TensorFlow, PyTorch, NumPy, Tkinter.
Hyperparameter Optimization: Good values for hyperparameters will be obtained using Optuna.
Data Representation: The environment and maze are represented as np.array.



### Game Class
- Focus on creating a `Game` class to simulate an environment with actors.
- Initial focus on predefined movements, with flexibility for random or predefined actor moves.


### Actor Class
- Allows testing of specific movement patterns to ensure expected actor behavior.
- Actors are prevented from moving into obstacle positions.
- `Actor` class initialized with starting position and movement speed.
- `move` method updates position based on direction `(dx, dy)`.
- `scan` method checks adjacent cells for obstacles, exits, or other actors.
- `make_decision` method uses scan results to choose next move, defaulting to random moves if not blocked.
- `step` method simulates a time step by scanning, deciding, and moving.

### Environment Class
- `Environment` class manages a 2D grid representing the game.
- Grid initialized as a 2D numpy array with cells for empty spaces, obstacles, exits, and actors.
- Methods to add obstacles (`add_obstacle`) and set an exit (`set_exit`) on the grid.
- Obstacles marked as `#`, exits as `E`.
- `place_actor` method for placing actors (hunter/prey) on the grid.
- Unique IDs used: `H` for hunters, `P` for prey.
- `display` method visualizes the grid using symbols for elements (e.g., `.` for empty, `#` for obstacles, `E` for exit).





